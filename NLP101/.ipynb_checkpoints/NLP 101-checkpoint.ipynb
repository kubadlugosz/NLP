{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob nltk seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:52:28.159044Z",
     "start_time": "2019-02-01T22:52:28.156036Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:51:29.649009Z",
     "start_time": "2019-02-01T22:51:29.637980Z"
    }
   },
   "outputs": [],
   "source": [
    "# check our working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our working directory to where our files are located\n",
    "##os.chdir('C:\\\\PATH\\\\PATH\\\\PATH\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:30.909758Z",
     "start_time": "2019-02-01T23:07:30.872662Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe from Yelp reviews file\n",
    "df = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:31.294593Z",
     "start_time": "2019-02-01T23:07:31.279555Z"
    }
   },
   "outputs": [],
   "source": [
    "# inspect the file\n",
    "print(df.describe())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:31.933024Z",
     "start_time": "2019-02-01T23:07:31.927007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select column with the review text from dataframe and turn into a list\n",
    "corpora = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.585179Z",
     "start_time": "2019-02-01T23:07:39.567142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view the first review\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set exhibit to the first review\n",
    "exhibit = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "docClean = \" \".join(x.lower() for x in exhibit.split())\n",
    "docClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.610246Z",
     "start_time": "2019-02-01T23:07:39.587184Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "docClean = re.sub('[0-9]', '', docClean).strip()\n",
    "docClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {'didn\\'t': 'did not','don\\'t': 'do not',\n",
    "                     'wouldn\\'t': 'would not', 'won\\'t': 'will not',\n",
    "                    'can\\'t': 'cannot', 'i\\'ve': 'i have', 'i\\'m': 'i am'}\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    #import re\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "def replace(match):\n",
    "    return contractions_dict[match.group(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_contractions('won\\'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docClean = expand_contractions(docClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize into sentences\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(docClean)\n",
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize into words\n",
    "#from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(docClean)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.922075Z",
     "start_time": "2019-02-01T23:07:39.706501Z"
    }
   },
   "source": [
    "### Pre-processing: Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "stopWords = list(set(stopwords.words('english')))\n",
    "\n",
    "#customize stop word list\n",
    "stopWords.append('food')\n",
    "\n",
    "#remove stop words from text\n",
    "cleanWords = [w for w in words if not w in stopWords]\n",
    "cleanWords[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Special Characters & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.644336Z",
     "start_time": "2019-02-01T23:07:39.612250Z"
    }
   },
   "outputs": [],
   "source": [
    "# special characters\n",
    "cleanWords = [w for w in cleanWords if re.search('^[a-zA-Z]+', w)]\n",
    "cleanWords[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "#import string\n",
    "cleanWords = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)), cleanWords))\n",
    "cleanWords[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Difference between Raw and Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'rawWords': words[:10], 'cleanWords': cleanWords[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing 3: Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#establish stemmer\n",
    "#from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "#stem words\n",
    "stemmedWords = [stemmer.stem(w) for w in cleanWords]\n",
    "pd.DataFrame({'cleanWords': cleanWords[:10], 'stemmedWords': stemmedWords[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function to convert pos_tag output to lemmatizer.lemmatize-friendly input ('NN' -> 'n')\n",
    "wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "wnpos('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#establish lemmatizer\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#lemmatize words\n",
    "lemWords = [lemmatizer.lemmatize(w, wnpos(pos_tag(w.split())[0][1])) for w in cleanWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cleanWords': cleanWords[:10], 'stemmedWords': stemmedWords[:10], 'lemWords': lemWords[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to apply all elements of text cleaning\n",
    "def clean_text(document):\n",
    "    #import statements\n",
    "    #from nltk.tokenize import word_tokenize\n",
    "    #from nltk.corpus import stopwords\n",
    "    document = str(document)\n",
    "    docClean = document.replace('\\n', ' ').replace('\\r', '')  ## Newline removal\n",
    "    docClean = \" \".join(x.lower() for x in docClean.split()) ## Lowercase\n",
    "    docClean = expand_contractions(docClean) # expand contractions\n",
    "    docClean = word_tokenize(docClean) #Tokenize\n",
    "    docClean = [w for w in docClean if not w in stopWords] #Drop Stop words\n",
    "    docClean = ' '.join([re.sub(r'\\W+','',w) for w in docClean]) #Remove non alphanumeric chars\n",
    "    docClean = re.sub('  ', ' ', docClean)\n",
    "    \n",
    "    return docClean.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, maxNGram):\n",
    "    text = text.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(text)-maxNGram+1):\n",
    "        output.append(text[i:i+maxNGram])\n",
    "    \n",
    "    return [' '.join(x) for x in output]\n",
    "\n",
    "def flat_list(x):\n",
    "    return [item for sublist in x for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigrams\n",
    "df['unigrams'] = df['cleanText'].apply(lambda x: ngrams(x, 1))\n",
    "# create bigrams\n",
    "df['bigrams'] = ...\n",
    "# create trigrams\n",
    "df['trigrams'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['unigrams'].tolist())).value_counts()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['bigrams'].tolist())).value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['trigrams'].tolist())).value_counts()[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:08:04.438331Z",
     "start_time": "2019-02-01T23:07:46.318097Z"
    }
   },
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:08:06.274218Z",
     "start_time": "2019-02-01T23:08:04.440337Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculates the sentiment, or polarity of a body of text\n",
    "# Output: sentiment - polarity score, scaled (-1, 1), of a document (higher == more positive)\n",
    "def get_sentiment(document):\n",
    "    try:\n",
    "        #from textblob import TextBlob\n",
    "        #drop non-alpha, keep some punctuation in raw text\n",
    "        document = re.sub('[^a-z0-0\\.?!\\',]', ' ', document.lower())\n",
    "        blob = TextBlob(document.lower())\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        \n",
    "        return sentiment\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to df to calculate sentiment\n",
    "df['sentiment'] =..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with a histogram\n",
    "sns.distplot(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowest sentiment statement\n",
    "df.text[df.sentiment==min(df.sentiment)].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates subjectivity, or modality, of a body of text\n",
    "# Output: subjectivity - modality score, scaled (0, 1), of a document (higher == more subjective)\n",
    "def get_subjectivity(document):\n",
    "    #from textblob import TextBlob\n",
    "    try:\n",
    "        #drop non-alpha, keep some punctuation in raw text\n",
    "        document = re.sub('[^a-z0-0\\.?!\\',]', ' ', document.lower())\n",
    "        blob = TextBlob(document.lower())\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        return subjectivity\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to df to calculate subjectivity\n",
    "df['subjectivity'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with a histogram\n",
    "sns.distplot(df['subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'sentiment', 'subjectivity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's find the review with the lowest subjectivity\n",
    "df.text[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the highest subjectivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot of Sentiment v. Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with a scatterplot\n",
    "plt.scatter(df['sentiment'], df['subjectivity'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:12:28.244216Z",
     "start_time": "2019-02-01T23:12:28.031650Z"
    }
   },
   "outputs": [],
   "source": [
    "#functions to calculate number of syllables\n",
    "#cmu dictionary\n",
    "d = cmudict.dict()\n",
    "\n",
    "#function to determine the number of syllables in a word with backup function if word not found in cmu dictionary\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)\n",
    "\n",
    "#backup function to determine syllables if word not found in cmu dictionary\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "        for index in range(1,len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count +=1\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        if word.endswith('le'):\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "    except IndexError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate total number of words\n",
    "def total_words(document):\n",
    "    #from nltk.tokenize import word_tokenize\n",
    "    words = word_tokenize(document)\n",
    "    return len(words)\n",
    "\n",
    "#function to calculate total number of sentences\n",
    "def total_sentences(document):\n",
    "    #from nltk.tokenize import sent_tokenize\n",
    "    sent = sent_tokenize(document)\n",
    "    return len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:14:15.465671Z",
     "start_time": "2019-02-01T23:14:15.304243Z"
    }
   },
   "outputs": [],
   "source": [
    "# create column for total words\n",
    "df['total_words'] = df['text'].apply(total_words)\n",
    "# create column for total sentences\n",
    "df['total_sentences'] = df['text'].apply(total_sentences)\n",
    "# create column for total syllables\n",
    "df['total_syllables'] = df['text'].apply(nsyl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flesch Reading Ease\n",
    "206.835-1.015\\*(total_words/total_sentences)-84.6*(total_syllables/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:21:55.732835Z",
     "start_time": "2019-02-01T23:21:55.729827Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate Flesch Reading Ease across the df\n",
    "def FRES(document):\n",
    "    try:\n",
    "        numWord = total_words(document)\n",
    "        numSent = total_sentences(document)\n",
    "        numSyll = nsyl(document)\n",
    "        x = (numWord / numSent)\n",
    "        y = (numSyll / numWord)\n",
    "        FRES = 206.835 - 1.015*(x) - 84.6*(y)\n",
    "        return FRES\n",
    "    except:\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply FRES function\n",
    "df['reading_ease'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reading_ease'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with histogram\n",
    "df['reading_ease'] = df['text'].apply(FRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowest reading ease\n",
    "df.text[df.reading_ease==min(df.reading_ease)].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:15:16.912478Z",
     "start_time": "2019-02-01T23:15:16.899444Z"
    }
   },
   "source": [
    "#### Flesch-Kincaid Grade\n",
    ".39*(total_words/total_sentences)+11.8*(total_syllables/total_words)-15.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:16:00.607563Z",
     "start_time": "2019-02-01T23:15:40.849451Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate Flesch-Kincaid grade level across the df\n",
    "def FKR(document):\n",
    "    try:\n",
    "        numWord = total_words(document)\n",
    "        numSent = total_sentences(document)\n",
    "        numSyll = nsyl(document)\n",
    "        x = (numWord / numSent)\n",
    "        y = (numSyll / numWord)\n",
    "        FKR = .39*(x) + 11.8*(y) - 15.59\n",
    "        return FKR\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:16:25.425570Z",
     "start_time": "2019-02-01T23:16:25.420556Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply FKR function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grade_level'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:19:13.388638Z",
     "start_time": "2019-02-01T23:19:13.383625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize with histogram\n",
    "sns.distplot(df['grade_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Using the provided _Literature_ dataset, complete the below exercises to conduct an exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is E878-394F\n",
      "\n",
      " Directory of C:\\Users\\jheuer\\Desktop\\auNLPTraining\\NLP 101\n",
      "\n",
      "07/24/2020  11:59 AM    <DIR>          .\n",
      "07/24/2020  11:59 AM    <DIR>          ..\n",
      "07/24/2020  12:53 AM    <DIR>          .ipynb_checkpoints\n",
      "04/30/2019  02:54 PM             2,048 literature.csv\n",
      "07/24/2020  11:59 AM            26,591 NLP 101.ipynb\n",
      "07/23/2020  07:57 PM        10,750,489 NLP 101.pptx\n",
      "04/30/2019  02:54 PM            15,386 NLP 101.py\n",
      "07/24/2020  11:59 AM            26,018 NLP 101-key.ipynb\n",
      "04/30/2019  02:54 PM               546 README.md\n",
      "04/30/2019  02:54 PM         1,606,396 yelp.csv\n",
      "               7 File(s)     12,427,474 bytes\n",
      "               3 Dir(s)  248,026,525,696 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:08.495892Z",
     "start_time": "2019-02-01T22:54:08.474837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a corpora and instantiate a Corpus class\n",
    "dfLit = pd.read_csv('Literature.csv')\n",
    "# Print the first document of the Corpus\n",
    "dfLit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness it was the epoch of belief it was the epoch of incredulity it was the season of light it was the season of darkness it was the spring of hope it was the winter of despair.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLit['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:08.927466Z",
     "start_time": "2019-02-01T22:54:08.921451Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many words are in your corpus? (hint: print the words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:09.383585Z",
     "start_time": "2019-02-01T22:54:09.375561Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate and print the clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:10.201378Z",
     "start_time": "2019-02-01T22:54:10.197365Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the bigrams of your corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Find the 10 top occuring words in your corpus and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:11.813885Z",
     "start_time": "2019-02-01T22:54:11.582270Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the top words in the corpus\n",
    "\n",
    "\n",
    "# get just the top 10 words and their counts\n",
    "\n",
    "\n",
    "# create plot of these words and their counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Return a sentiment score from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:13.113690Z",
     "start_time": "2019-02-01T22:54:12.821915Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sentiment scores for each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:13.449581Z",
     "start_time": "2019-02-01T22:54:13.278141Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the sentiment distribution of the documents in the corpus\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "958px",
    "top": "146px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
