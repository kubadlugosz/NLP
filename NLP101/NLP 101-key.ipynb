{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:52:28.159044Z",
     "start_time": "2019-02-01T22:52:28.156036Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:51:29.649009Z",
     "start_time": "2019-02-01T22:51:29.637980Z"
    }
   },
   "outputs": [],
   "source": [
    "# check our working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:30.909758Z",
     "start_time": "2019-02-01T23:07:30.872662Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe from Yelp reviews file\n",
    "df = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:31.294593Z",
     "start_time": "2019-02-01T23:07:31.279555Z"
    }
   },
   "outputs": [],
   "source": [
    "# inspect the file\n",
    "print(df.describe())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:31.933024Z",
     "start_time": "2019-02-01T23:07:31.927007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select column with the review text from dataframe and turn into a list\n",
    "corpora = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.585179Z",
     "start_time": "2019-02-01T23:07:39.567142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpora[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exhibit = corpora[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "docClean = \" \".join(x.lower() for x in exhibit.split())\n",
    "docClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.610246Z",
     "start_time": "2019-02-01T23:07:39.587184Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove numbers\n",
    "docClean = re.sub('[0-9]', '', docClean).strip()\n",
    "docClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {'didn\\'t': 'did not','don\\'t': 'do not',\n",
    "                     'wouldn\\'t': 'would not', 'won\\'t': 'will not',\n",
    "                    'can\\'t': 'cannot', 'i\\'ve': 'i have', 'i\\'m': 'i am'}\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    #import re\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "def replace(match):\n",
    "    return contractions_dict[match.group(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_contractions('won\\'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docClean = expand_contractions(docClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize into sentences\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(docClean)\n",
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize into words\n",
    "#from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(docClean)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.922075Z",
     "start_time": "2019-02-01T23:07:39.706501Z"
    }
   },
   "source": [
    "### Pre-processing: Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "stopWords = list(set(stopwords.words('english')))\n",
    "\n",
    "#customize stop word list\n",
    "stopWords.append('food')\n",
    "\n",
    "#remove stop words from text\n",
    "cleanWords = [w for w in words if not w in stopWords]\n",
    "cleanWords[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Special Characters & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:07:39.644336Z",
     "start_time": "2019-02-01T23:07:39.612250Z"
    }
   },
   "outputs": [],
   "source": [
    "# special characters\n",
    "cleanWords = [w for w in cleanWords if re.search('^[a-zA-Z]+', w)]\n",
    "cleanWords[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "#import string\n",
    "cleanWords = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)), cleanWords))\n",
    "cleanWords[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Difference between Raw and Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'rawWords': words[:10], 'cleanWords': cleanWords[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing 3: Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#establish stemmer\n",
    "#from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "#stem words\n",
    "stemmedWords = [stemmer.stem(w) for w in cleanWords]\n",
    "pd.DataFrame({'cleanWords': cleanWords[:10], 'stemmedWords': stemmedWords[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function to convert pos_tag output to lemmatizer.lemmatize-friendly input ('NN' -> 'n')\n",
    "wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "wnpos('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#establish lemmatizer\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#lemmatize words\n",
    "lemWords = [lemmatizer.lemmatize(w, wnpos(pos_tag(w.split())[0][1])) for w in cleanWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cleanWords': cleanWords[:10], 'stemmedWords': stemmedWords[:10], 'lemWords': lemWords[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to apply all elements of text cleaning\n",
    "def clean_text(document):\n",
    "    #import statements\n",
    "    #from nltk.tokenize import word_tokenize\n",
    "    #from nltk.corpus import stopwords\n",
    "    document = str(document)\n",
    "    docClean = document.replace('\\n', ' ').replace('\\r', '')  ## Newline removal\n",
    "    docClean = \" \".join(x.lower() for x in docClean.split()) ## Lowercase\n",
    "    docClean = expand_contractions(docClean) # expand contractions\n",
    "    docClean = word_tokenize(docClean) #Tokenize\n",
    "    docClean = [w for w in docClean if not w in stopWords] #Drop Stop words\n",
    "    docClean = ' '.join([re.sub(r'\\W+','',w) for w in docClean]) #Remove non alphanumeric chars\n",
    "    docClean = re.sub('  ', ' ', docClean)\n",
    "    \n",
    "    return docClean.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, maxNGram):\n",
    "    text = text.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(text)-maxNGram+1):\n",
    "        output.append(text[i:i+maxNGram])\n",
    "    \n",
    "    return [' '.join(x) for x in output]\n",
    "\n",
    "def flat_list(x):\n",
    "    return [item for sublist in x for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unigrams'] = df['cleanText'].apply(lambda x: ngrams(x, 1))\n",
    "df['bigrams'] = df['cleanText'].apply(lambda x: ngrams(x, 2))\n",
    "df['trigrams'] = df['cleanText'].apply(lambda x: ngrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['unigrams'].tolist())).value_counts()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['bigrams'].tolist())).value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list(df['trigrams'].tolist())).value_counts()[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:08:04.438331Z",
     "start_time": "2019-02-01T23:07:46.318097Z"
    }
   },
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:08:06.274218Z",
     "start_time": "2019-02-01T23:08:04.440337Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculates the sentiment, or polarity of a body of text\n",
    "# Output: sentiment - polarity score, scaled (-1, 1), of a document (higher == more positive)\n",
    "def get_sentiment(document):\n",
    "    try:\n",
    "        #from textblob import TextBlob\n",
    "        #drop non-alpha, keep some punctuation in raw text\n",
    "        document = re.sub('[^a-z0-0\\.?!\\',]', ' ', document.lower())\n",
    "        blob = TextBlob(document.lower())\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        \n",
    "        return sentiment\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to df to calculate sentiment\n",
    "df['sentiment'] = df['text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates subjectivity, or modality, of a body of text\n",
    "# Output: subjectivity - modality score, scaled (0, 1), of a document (higher == more subjective)\n",
    "def get_subjectivity(document):\n",
    "    #from textblob import TextBlob\n",
    "    try:\n",
    "        #drop non-alpha, keep some punctuation in raw text\n",
    "        document = re.sub('[^a-z0-0\\.?!\\',]', ' ', document.lower())\n",
    "        blob = TextBlob(document.lower())\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        return subjectivity\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to df to calculate subjectivity\n",
    "df['subjectivity'] = df['text'].apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subjectivity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'sentiment', 'subjectivity']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot of Sentiment v. Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['sentiment'], df['subjectivity'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Subjectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:12:28.244216Z",
     "start_time": "2019-02-01T23:12:28.031650Z"
    }
   },
   "outputs": [],
   "source": [
    "#functions to calculate number of syllables\n",
    "#cmu dictionary\n",
    "d = cmudict.dict()\n",
    "\n",
    "#function to determine the number of syllables in a word with backup function if word not found in cmu dictionary\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)\n",
    "\n",
    "#backup function to determine syllables if word not found in cmu dictionary\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "        for index in range(1,len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count +=1\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        if word.endswith('le'):\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "    except IndexError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate total number of words\n",
    "def total_words(document):\n",
    "    #from nltk.tokenize import word_tokenize\n",
    "    words = word_tokenize(document)\n",
    "    return len(words)\n",
    "\n",
    "#function to calculate total number of sentences\n",
    "def total_sentences(document):\n",
    "    #from nltk.tokenize import sent_tokenize\n",
    "    sent = sent_tokenize(document)\n",
    "    return len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:14:15.465671Z",
     "start_time": "2019-02-01T23:14:15.304243Z"
    }
   },
   "outputs": [],
   "source": [
    "df['total_words'] = df['text'].apply(total_words)\n",
    "df['total_sentences'] = df['text'].apply(total_sentences)\n",
    "df['total_syllables'] = df['text'].apply(nsyl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flesch Reading Ease\n",
    "206.835-1.015\\*(total_words/total_sentences)-84.6*(total_syllables/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:21:55.732835Z",
     "start_time": "2019-02-01T23:21:55.729827Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate Flesch Reading Ease across the df\n",
    "def FRES(document):\n",
    "    try:\n",
    "        numWord = total_words(document)\n",
    "        numSent = total_sentences(document)\n",
    "        numSyll = nsyl(document)\n",
    "        x = (numWord / numSent)\n",
    "        y = (numSyll / numWord)\n",
    "        FRES = 206.835 - 1.015*(x) - 84.6*(y)\n",
    "        return FRES\n",
    "    except:\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reading_ease'] = df['text'].apply(FRES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reading_ease'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['reading_ease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:15:16.912478Z",
     "start_time": "2019-02-01T23:15:16.899444Z"
    }
   },
   "source": [
    "#### Flesch-Kincaid Grade\n",
    ".39*(total_words/total_sentences)+11.8*(total_syllables/total_words)-15.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:16:00.607563Z",
     "start_time": "2019-02-01T23:15:40.849451Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate Flesch-Kincaid grade level across the df\n",
    "def FKR(document):\n",
    "    try:\n",
    "        numWord = total_words(document)\n",
    "        numSent = total_sentences(document)\n",
    "        numSyll = nsyl(document)\n",
    "        x = (numWord / numSent)\n",
    "        y = (numSyll / numWord)\n",
    "        FKR = .39*(x) + 11.8*(y) - 15.59\n",
    "        return FKR\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:16:25.425570Z",
     "start_time": "2019-02-01T23:16:25.420556Z"
    }
   },
   "outputs": [],
   "source": [
    "df['grade_level'] = df['text'].apply(FKR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['grade_level'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:19:13.388638Z",
     "start_time": "2019-02-01T23:19:13.383625Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['grade_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Using the provided _Literature_ dataset, complete the below exercises to conduct an exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:08.495892Z",
     "start_time": "2019-02-01T22:54:08.474837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a corpora and instantiate a Corpus class\n",
    "dfLit = pd.read_csv('Literature.csv')\n",
    "# Print the first document of the Corpus\n",
    "dfLit['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:08.927466Z",
     "start_time": "2019-02-01T22:54:08.921451Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many words are in your corpus? (hint: print the words)\n",
    "dfLit['words'] = dfLit['text'].apply(lambda x: ngrams(x, 1))\n",
    "print('count:', len(flat_list(dfLit['words'].tolist())))\n",
    "print('unique:', len(set(flat_list(dfLit['words'].tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = ['and', 'or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:09.383585Z",
     "start_time": "2019-02-01T22:54:09.375561Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate and print the clean text\n",
    "dfLit['cleanText'] = dfLit['text'].apply(clean_text)\n",
    "dfLit['cleanText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:10.201378Z",
     "start_time": "2019-02-01T22:54:10.197365Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the bigrams of your corpus\n",
    "dfLit['bigrams'] = dfLit['cleanText'].apply(lambda x: ngrams(x, 2))\n",
    "pd.Series(flat_list(dfLit['bigrams'].tolist())).value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Find the 10 top occuring words in your corpus and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:11.813885Z",
     "start_time": "2019-02-01T22:54:11.582270Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the top words in the corpus\n",
    "allWordCounts = pd.Series(flat_list(dfLit['words'].tolist())).value_counts()[:10]\n",
    "\n",
    "# get just the top 10 words and their counts\n",
    "topWords = allWordCounts[:10]\n",
    "\n",
    "# create plot of these words and their counts\n",
    "sns.barplot(topWords.index, topWords.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Return a sentiment score from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:13.113690Z",
     "start_time": "2019-02-01T22:54:12.821915Z"
    }
   },
   "outputs": [],
   "source": [
    "# get sentiment scores for each document\n",
    "dfLit['sentiment'] = dfLit['cleanText'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfLit['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLit[dfLit['sentiment'] == dfLit['sentiment'].min()]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T22:54:13.449581Z",
     "start_time": "2019-02-01T22:54:13.278141Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the sentiment distribution of the documents in the corpus\n",
    "sns.distplot(dfLit['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "958px",
    "top": "146px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
